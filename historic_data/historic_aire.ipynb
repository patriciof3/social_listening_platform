{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementNotInteractableException, ElementClickInterceptedException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = {\n",
    "    \"litoral\": {\n",
    "        \"violencia_narco\": \"https://www.ellitoral.com/tag/violencia-narco\",\n",
    "        \"narcotrafico\": \"https://www.ellitoral.com/tag/narcotrafico\",\n",
    "    },\n",
    "    \"aire\": {\n",
    "        \"droga\": \"https://www.airedesantafe.com.ar/droga-a848/\",\n",
    "        \"narcotrafico\": \"https://www.airedesantafe.com.ar/narcotrafico-a534\",\n",
    "        \"cocaina\": \"https://www.airedesantafe.com.ar/cocaina-a1378\",\n",
    "        \"drogas\": \"https://www.airedesantafe.com.ar/drogas-a4787\",\n",
    "    },\n",
    "    \"rosario12\": {\n",
    "        \"general\": \"https://www.pagina12.com.ar/suplementos/rosario12/{date}\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import (\n",
    "    NoSuchElementException, \n",
    "    ElementNotInteractableException, \n",
    "    ElementClickInterceptedException\n",
    ")\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_with_multiple_clicks_aire(sources):\n",
    "    # Path to your Brave executable\n",
    "    brave_path = r\"C:\\Program Files\\BraveSoftware\\Brave-Browser\\Application\\brave.exe\"\n",
    "\n",
    "    # Set up Chrome options\n",
    "    options = Options()\n",
    "    options.binary_location = brave_path\n",
    "\n",
    "    # Create a WebDriver instance\n",
    "    driver = webdriver.Chrome(options=options)  \n",
    "\n",
    "    # List to store all scraped data\n",
    "    data = []\n",
    "\n",
    "    # Extract URLs and tags\n",
    "    urls = list(sources[\"aire\"].values())\n",
    "    tags = list(sources[\"aire\"].keys())\n",
    "\n",
    "    try:\n",
    "        # Iterate through each URL and tag\n",
    "        for url, tag in zip(urls, tags):\n",
    "            try:\n",
    "                # Load the webpage\n",
    "                driver.get(url)\n",
    "                original_url = driver.current_url  # Store the original URL\n",
    "                print(f\"Processing URL: {url}\")\n",
    "\n",
    "                # Wait for the page to load initially\n",
    "                time.sleep(5)\n",
    "\n",
    "                # Handle cancel buttons if present\n",
    "                try:\n",
    "                    cancel_button = driver.find_element(By.ID, \"onesignal-slidedown-cancel-button\")\n",
    "                    cancel_button.click()\n",
    "                    time.sleep(2)\n",
    "                except NoSuchElementException:\n",
    "                    print(\"Cancel button not found.\")\n",
    "                \n",
    "                # Start scraping loop\n",
    "                while True:\n",
    "                    # Extract page content\n",
    "                    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                    divs = soup.find_all('div', class_='article-title')\n",
    "                    \n",
    "                    for div in divs:\n",
    "                        a_tag = div.find('a', class_='a-article-link')\n",
    "                        if a_tag:\n",
    "                            href = a_tag['href']\n",
    "                            title = a_tag.get_text(strip=True)\n",
    "                            data.append({'Link': href, 'Title': title, 'Tag': tag, 'Media': 'aire'})\n",
    "\n",
    "                    print(f\"Scraped {len(data)} links so far.\")\n",
    "                            \n",
    "                    # Click the next button\n",
    "                    try:\n",
    "                        next_button = driver.find_element(By.CSS_SELECTOR, \".next.show a\")\n",
    "                        driver.execute_script(\"arguments[0].scrollIntoView();\", next_button)\n",
    "                        next_button.click()\n",
    "                        time.sleep(5)  # Wait for new content to load\n",
    "                    except NoSuchElementException:\n",
    "                        print(\"Next button not found. Exiting loop.\")\n",
    "                        break\n",
    "                    except (ElementNotInteractableException, ElementClickInterceptedException) as e:\n",
    "                        print(f\"Button click error: {e}\")\n",
    "                        break\n",
    "\n",
    "                    # Stop if redirected back to the original URL\n",
    "                    if driver.current_url == original_url:\n",
    "                        print(\"Redirected back to the original URL. Breaking loop.\")\n",
    "                        break\n",
    "\n",
    "                    # Handle cancel buttons again if they reappear\n",
    "                    try:\n",
    "                        cancel_button = driver.find_element(By.ID, \"onesignal-slidedown-cancel-button\")\n",
    "                        cancel_button.click()\n",
    "                        time.sleep(2)\n",
    "                    except NoSuchElementException:\n",
    "                        pass\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "    # Create a DataFrame from the data list\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing URL: https://www.airedesantafe.com.ar/droga-a848/\n",
      "Scraped 10 links so far.\n",
      "Scraped 20 links so far.\n",
      "Scraped 30 links so far.\n",
      "Scraped 40 links so far.\n",
      "Scraped 50 links so far.\n",
      "Scraped 60 links so far.\n",
      "Scraped 70 links so far.\n",
      "Scraped 80 links so far.\n",
      "Scraped 90 links so far.\n",
      "Scraped 100 links so far.\n",
      "Scraped 110 links so far.\n",
      "Scraped 120 links so far.\n",
      "Scraped 130 links so far.\n",
      "Scraped 133 links so far.\n",
      "Next button not found. Exiting loop.\n",
      "Processing URL: https://www.airedesantafe.com.ar/narcotrafico-a534\n",
      "Cancel button not found.\n",
      "Scraped 143 links so far.\n",
      "Scraped 153 links so far.\n",
      "Scraped 163 links so far.\n",
      "Scraped 173 links so far.\n",
      "Scraped 183 links so far.\n",
      "Scraped 193 links so far.\n",
      "Scraped 203 links so far.\n",
      "Scraped 213 links so far.\n",
      "Scraped 223 links so far.\n",
      "Scraped 233 links so far.\n",
      "Scraped 243 links so far.\n",
      "Scraped 253 links so far.\n",
      "Scraped 263 links so far.\n",
      "Scraped 273 links so far.\n",
      "Scraped 283 links so far.\n",
      "Scraped 293 links so far.\n",
      "Scraped 303 links so far.\n",
      "Scraped 313 links so far.\n",
      "Scraped 323 links so far.\n",
      "Scraped 333 links so far.\n",
      "Scraped 343 links so far.\n",
      "Scraped 353 links so far.\n",
      "Scraped 363 links so far.\n",
      "Scraped 373 links so far.\n",
      "Scraped 383 links so far.\n",
      "Scraped 393 links so far.\n",
      "Scraped 403 links so far.\n",
      "Scraped 413 links so far.\n",
      "Scraped 423 links so far.\n",
      "Scraped 433 links so far.\n",
      "Scraped 443 links so far.\n",
      "Scraped 453 links so far.\n",
      "Scraped 463 links so far.\n",
      "Scraped 473 links so far.\n",
      "Scraped 483 links so far.\n",
      "Scraped 493 links so far.\n",
      "Scraped 503 links so far.\n",
      "Scraped 513 links so far.\n",
      "Scraped 514 links so far.\n",
      "Next button not found. Exiting loop.\n",
      "Processing URL: https://www.airedesantafe.com.ar/cocaina-a1378\n",
      "Cancel button not found.\n",
      "Scraped 524 links so far.\n",
      "Scraped 534 links so far.\n",
      "Scraped 544 links so far.\n",
      "Scraped 554 links so far.\n",
      "Scraped 564 links so far.\n",
      "Scraped 574 links so far.\n",
      "Scraped 584 links so far.\n",
      "Scraped 594 links so far.\n",
      "Scraped 604 links so far.\n",
      "Scraped 614 links so far.\n",
      "Scraped 624 links so far.\n",
      "Scraped 634 links so far.\n",
      "Scraped 644 links so far.\n",
      "Scraped 654 links so far.\n",
      "Scraped 664 links so far.\n",
      "Scraped 674 links so far.\n",
      "Scraped 684 links so far.\n",
      "Scraped 694 links so far.\n",
      "Scraped 704 links so far.\n",
      "Scraped 711 links so far.\n",
      "Next button not found. Exiting loop.\n",
      "Processing URL: https://www.airedesantafe.com.ar/drogas-a4787\n",
      "Cancel button not found.\n",
      "Scraped 721 links so far.\n",
      "Scraped 731 links so far.\n",
      "Scraped 741 links so far.\n",
      "Scraped 751 links so far.\n",
      "Scraped 761 links so far.\n",
      "Scraped 771 links so far.\n",
      "Scraped 781 links so far.\n",
      "Scraped 791 links so far.\n",
      "Scraped 801 links so far.\n",
      "Scraped 811 links so far.\n",
      "Scraped 821 links so far.\n",
      "Scraped 831 links so far.\n",
      "Scraped 841 links so far.\n",
      "Scraped 851 links so far.\n",
      "Scraped 861 links so far.\n",
      "Scraped 871 links so far.\n",
      "Scraped 881 links so far.\n",
      "Scraped 882 links so far.\n",
      "Next button not found. Exiting loop.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df_aire = scrape_with_multiple_clicks_aire(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aire.to_excel(\"data/df_aire_historic.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Link</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Media</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.airedesantafe.com.ar/policiales/a-...</td>\n",
       "      <td>Microtráfico: en un año, se derrumbaron 45 bún...</td>\n",
       "      <td>droga</td>\n",
       "      <td>aire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.airedesantafe.com.ar/policiales/un...</td>\n",
       "      <td>Un policía de San Javier fue detenido por camb...</td>\n",
       "      <td>droga</td>\n",
       "      <td>aire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.airedesantafe.com.ar/actualidad/ju...</td>\n",
       "      <td>Jujuy: un nene de 4 años fue internado por con...</td>\n",
       "      <td>droga</td>\n",
       "      <td>aire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.airedesantafe.com.ar/actualidad/sa...</td>\n",
       "      <td>Santiago del Estero: discutió con su esposo na...</td>\n",
       "      <td>droga</td>\n",
       "      <td>aire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.airedesantafe.com.ar/policiales/lo...</td>\n",
       "      <td>Los \"paranze\" rosarinos: jóvenes mafiosos que ...</td>\n",
       "      <td>droga</td>\n",
       "      <td>aire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Link  \\\n",
       "0  https://www.airedesantafe.com.ar/policiales/a-...   \n",
       "1  https://www.airedesantafe.com.ar/policiales/un...   \n",
       "2  https://www.airedesantafe.com.ar/actualidad/ju...   \n",
       "3  https://www.airedesantafe.com.ar/actualidad/sa...   \n",
       "4  https://www.airedesantafe.com.ar/policiales/lo...   \n",
       "\n",
       "                                               Title    Tag Media  \n",
       "0  Microtráfico: en un año, se derrumbaron 45 bún...  droga  aire  \n",
       "1  Un policía de San Javier fue detenido por camb...  droga  aire  \n",
       "2  Jujuy: un nene de 4 años fue internado por con...  droga  aire  \n",
       "3  Santiago del Estero: discutió con su esposo na...  droga  aire  \n",
       "4  Los \"paranze\" rosarinos: jóvenes mafiosos que ...  droga  aire  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aire = pd.read_excel(\"../data/df_aire_historic.xlsx\")\n",
    "df_aire.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def content_and_date_extract(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content.decode('utf-8', errors='ignore'), 'html.parser')\n",
    "        \n",
    "        # Extract paragraphs\n",
    "        paragraphs = soup.findAll('p')\n",
    "        content_list = [p.get_text() for p in paragraphs if p.get_text()]\n",
    "        content_list = content_list[:-2]\n",
    "        # Extract the <div> with the class 'article-date'\n",
    "        date_div = soup.find('div', class_='article-date')\n",
    "\n",
    "        # Extract datetime attribute\n",
    "        if date_div:\n",
    "            # Find the <time> tag within that div\n",
    "            datetime_element = date_div.find('time', attrs={\"datetime\": True})\n",
    "            if datetime_element:\n",
    "                extracted_date = datetime_element['datetime']  # Extract the value of the datetime attribute\n",
    "            else:\n",
    "                extracted_date = \"Date not found in time tag\"\n",
    "        else:\n",
    "            extracted_date = \"No article-date div found\"\n",
    "\n",
    "        # Return a dictionary with both content and date\n",
    "        return {\"content\": content_list if content_list else [\"No content found\"], \"date\": extracted_date}\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\"content\": [f\"Request error: {e}\"], \"date\": None}\n",
    "    except Exception as e:\n",
    "        return {\"content\": [f\"Error: {e}\"], \"date\": None}\n",
    "\n",
    "# Apply the function to the 'Link' column\n",
    "df_extracted = df_aire['Link'].apply(content_and_date_extract)\n",
    "\n",
    "# Create separate columns for 'content' and 'date'\n",
    "df_aire['content'] = df_extracted.apply(lambda x: x['content'])\n",
    "df_aire['date'] = df_extracted.apply(lambda x: x['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out items with less than 30 characters\n",
    "df_aire[\"content\"] = df_aire[\"content\"].apply(lambda x: [s for s in x if len(s) >= 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aire['date'] = pd.to_datetime(df_aire['date'], errors='coerce')\n",
    "df_aire['date'] = df_aire[\"date\"].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aire.to_excel(\"../data/df_aire_historic_content.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
